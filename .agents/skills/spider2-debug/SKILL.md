---
name: spider2-debug
description: 'Debug Spider2 crawl failures and data quality. Triggers: crawl error, empty fields, data issue.'
metadata:
  version: '1'
---

# Debugging Spider2

## ğŸš€ å¿«é€Ÿæ’æŸ¥æµç¨‹ï¼ˆå…ˆåšè¿™äº›å†æ·±å…¥ï¼‰

### Step 1: ç¡®å®šé—®é¢˜ç±»å‹

| é—®é¢˜æè¿° | å¿«é€Ÿå‘½ä»¤ | åˆ¤æ–­ä¾æ® |
| :--------- | :--------- | :--------- |
| å¢é‡æ•°æ®å­—æ®µä¸ºç©º | `duckdb -csv -c "SELECT ... empty_title"` | empty_title > 0 = åå°ç»´æŠ¤ |
| ä»»åŠ¡å¤±è´¥ | `pueue log <ID> \| tail -50` | çœ‹ ERROR å’Œç»Ÿè®¡ |
| æ•°æ®é‡å¼‚å¸¸ | `duckdb -csv -c "SELECT crawler, COUNT(*)"` | å¯¹æ¯”æ˜¨æ—¥ |

### Step 2: å¢é‡æ•°æ®è´¨é‡æ£€æŸ¥ï¼ˆæœ€å¸¸ç”¨ï¼‰

```bash
# ä¸€é”®æ£€æŸ¥æœ€æ–°å¢é‡æ‰¹æ¬¡çš„æ•°æ®è´¨é‡
duckdb -csv -c "
SELECT
  regexp_extract(product_url, 'dorasuta\.jp/([^/]+)/', 1) as game,
  COUNT(*) as total,
  SUM(CASE WHEN product_title = '' OR product_title IS NULL THEN 1 ELSE 0 END) as empty_title,
  SUM(CASE WHEN product_condition_list IS NULL OR len(product_condition_list) = 0 THEN 1 ELSE 0 END) as empty_condition
FROM 'dump_parquet/increment/$(ls -t dump_parquet/increment | head -1)/**/*.parquet'
GROUP BY 1
ORDER BY empty_title DESC
"
```

**è§£è¯»**ï¼š

- `empty_title = total` â†’ è¯¥ç±»ç›®åå°ç»´æŠ¤ï¼Œæ•´ä¸ªç±»ç›®å¤±è´¥
- `empty_title` å æ¯”å° â†’ éƒ¨åˆ†å•†å“ä¸‹æ¶
- `empty_condition > 0` ä½† `empty_title = 0` â†’ æ£€æŸ¥è§£æé€»è¾‘

### Step 3: å¿«é€Ÿå®šä½æ—¥å¿—

```bash
# æ‰¾åˆ°æŸæ‰¹æ¬¡çš„ pueue ä»»åŠ¡ ID
rg -l "<task_id_pattern>" ~/.local/share/pueue/task_logs/ | head -5

# æ£€æŸ¥è­¦å‘Šæ•°ï¼ˆç©ºå­—æ®µçš„å¿«é€ŸæŒ‡æ ‡ï¼‰
rg -c "æ²¡æœ‰è·å–åˆ°" ~/.local/share/pueue/task_logs/<ID>.log
```

**è­¦å‘Šæ•°åˆ¤æ–­**ï¼š

- è­¦å‘Šæ•° â‰ˆ scraped Ã— 2 â†’ å…¨éƒ¨å¤±è´¥ï¼ˆåå°ç»´æŠ¤ï¼‰
- è­¦å‘Šæ•° << scraped â†’ å¤§éƒ¨åˆ†æˆåŠŸ
- è­¦å‘Šæ•° = 0 â†’ å…¨éƒ¨æˆåŠŸ

## âš ï¸ å¸¸è§å¼¯è·¯ï¼ˆä¸è¦è¿™æ ·åšï¼‰

| âŒ å¼¯è·¯ | âœ… æ­£ç¡®åšæ³• | åŸå›  |
| :-------- | :----------- | :----- |
| ç”¨ polars/pandas å†™å¤æ‚ Python åˆ†æ | ç”¨ duckdb ä¸€è¡Œ SQL | duckdb ç›´æ¥è¯» parquetï¼Œæ›´å¿«æ›´ç®€æ´ |
| ç”¨ `os.environ['MONGODB_URI']` è¿ MongoDB | ç›´æ¥åˆ†æ `dump_parquet/` | parquet å·²æ˜¯å¯¼å‡ºæ•°æ®ï¼Œæ— éœ€è¿æ¥æ•°æ®åº“ |
| çœ‹åˆ°ç©ºå­—æ®µå°±æ€€ç–‘è§£æé€»è¾‘ | å…ˆæ£€æŸ¥ `empty_title` æ˜¯å¦ä¹Ÿä¸ºç©º | title ä¹Ÿç©º = æ•´ä¸ªè§£æå¤±è´¥ = åå°ç»´æŠ¤ |
| ç”¨ httpx/curl æµ‹è¯•é¡µé¢ | ç”¨ `scrapy fetch` | TLS æŒ‡çº¹ä¸åŒï¼Œcurl ä¼šè¢« Cloudflare æ‹¦æˆª |
| é€æ¡åˆ†æç©ºå­—æ®µ | å…ˆçœ‹æ—¥å¿—è­¦å‘Šæ•° | è­¦å‘Šæ•° â‰ˆ scraped Ã— 2 = åå°ç»´æŠ¤ |

## âš ï¸ å…³é”®èƒŒæ™¯çŸ¥è¯†

### ä»£ç†é…ç½®æ¥æº

**è¿è¡Œæ—¶é…ç½®çœ‹ `apps/cron/spider2/crawl_all/main.sh`ï¼Œä¸æ˜¯çˆ¬è™«ä»£ç ã€‚**

```bash
# æŸ¥çœ‹å®é™…è¿è¡Œé…ç½®
rg "<spider_name>" apps/cron/spider2/crawl_all/main.sh

# æœ‰ -s JHS_PROXY=cloudbypass â†’ ä½¿ç”¨ HTTP ä»£ç†
# æ—  JHS_PROXY â†’ ç›´è¿ï¼ˆå¦‚ dorasutaï¼‰
```

### ä»£ç†ç­–ç•¥é€ŸæŸ¥

| ç½‘ç«™ | ä»£ç†ç­–ç•¥ | å¤‡æ³¨ |
| ------ | ---------- | ------ |
| **dorasuta** | ç›´è¿ï¼ˆæ— ä»£ç†ï¼‰ | IP ç™½åå•ï¼Œé¾™æ˜Ÿå·²å¼€æ”¾ |
| cardrush | HTTP ä»£ç† | JHS_PROXY=cloudbypass |
| hareruya2 | HTTP ä»£ç† | JHS_PROXY=cloudbypass |
| hareruyamtg | HTTP ä»£ç† | JHS_PROXY=cloudbypass |
| mercari | HTTP ä»£ç† | JHS_PROXY=cloudbypass |
| yuyu-tei | HTTP ä»£ç† | JHS_PROXY=cloudbypass |

### ä¸¤ç§ä»£ç†æ¨¡å¼ï¼ˆä¸è¦æ··æ·†ï¼‰

| æ¨¡å¼ | è§¦å‘æ–¹å¼ | ç¯å¢ƒå˜é‡ | å·¥ä½œåŸç† |
| ------ | ---------- | ---------- | ---------- |
| **HTTP ä»£ç†** | `-s JHS_PROXY=cloudbypass` | `PROXY_CLOUDBYPASS_SERVER/USERNAME/PASSWORD` | è¯·æ±‚é€šè¿‡ HTTP ä»£ç†æœåŠ¡å™¨ |
| **API ä»£ç†** | `CloudBypassRequest(cloudbypass_enabled=True)` | `PROXY_CLOUDBYPASS_APIKEY/LONG` | è¯·æ±‚å‘é€åˆ° api.cloudbypass.com |

**dorasuta ç‰¹æ®Šæƒ…å†µ**ï¼šä»£ç ä¸­æœ‰ `CloudBypassRequest`ï¼Œä½† `cloudbypass_enabled=False`ï¼ˆé»˜è®¤ï¼‰ï¼Œå®é™…ä¸èµ° APIï¼Œç›´è¿ç›®æ ‡ç½‘ç«™ã€‚

### æµ‹è¯•å·¥å…·é€‰æ‹©

**å¿…é¡»ç”¨ `scrapy fetch`ï¼Œä¸èƒ½ç”¨ httpx/curl/requestsã€‚**

Cloudflare æ ¹æ® TLS fingerprint åŒºåˆ†å®¢æˆ·ç«¯ã€‚scrapy å’Œ httpx çš„æŒ‡çº¹ä¸åŒï¼š

- scrapy fetch â†’ é€šè¿‡ï¼ˆåŒ¹é…çˆ¬è™«ç¯å¢ƒï¼‰
- httpx/curl â†’ 403 "Just a moment..."ï¼ˆè¢«æ‹¦æˆªï¼‰

```bash
# æ­£ç¡®
cd apps/spider2
timeout 30 mise exec --env local -- uv run scrapy fetch "<URL>" -s LOG_LEVEL=ERROR 2>/dev/null > /tmp/page.html

# é”™è¯¯ï¼ˆä¼šè¯¯åˆ¤ä¸º Cloudflare å°ç¦ï¼‰
curl "https://dorasuta.jp/..."
python -c "import httpx; print(httpx.get('https://dorasuta.jp/...').status_code)"
```

### URL æ¥æº

**ä»çˆ¬è™«ä»£ç æå–ï¼Œä¸è¦çŒœæµ‹ã€‚**

```bash
# æ‰¾ URL å®šä¹‰
rg "base_url|start_urls" apps/spider2/src/spider2/spiders/<site>/
rg "def gen_request" apps/spider2/src/spider2/spiders/<site>/

# å¸¸è§é”™è¯¯
# âŒ hareruya.com â†’ âœ… hareruya2.com
# âŒ éšæ„æ„é€ å‚æ•° â†’ âœ… ä» gen_request æå–å®Œæ•´å‚æ•°
```

## Pueue JSON Schema

```bash
pueue status --json | jq '.tasks["<ID>"]'
```

å…³é”®å­—æ®µï¼š

- `.original_command` â†’ åŒ…å« `crawl <spider_name> -a task_id=<task_id>`
- `.status.Done.result` â†’ `"Success"` æˆ– `{"Failed": <code>}`
- `.created_at` / `.status.Done.start` / `.status.Done.end` â†’ æ—¶é—´æˆ³
- `.path` â†’ åŒ…å« `spider2` å¯ç”¨äºè¿‡æ»¤

æå– spider åç§°ï¼ˆjq è¡¨è¾¾å¼ï¼‰ï¼š

```bash
.original_command | capture("crawl (?<s>[^ ]+)").s
```

æŒ‰æ—¥æœŸæ±‡æ€»ï¼š

```bash
pueue status --json | jq '
  [.tasks | to_entries[]
   | select(.value.path | contains("spider2"))
   | select(.value.created_at > "'$(date +%Y-%m-%d)'")
  ] | group_by(.value.created_at | split("T")[0])
    | map({date: .[0].value.created_at | split("T")[0], total: length,
           success: [.[] | select(.value.status.Done.result == "Success")] | length})'
```

## Parquet Schema

```sql
SELECT site, crawler, task_id, COUNT(*)
FROM 'dump_parquet/all/YYMMDD/**/*.parquet'
GROUP BY 1,2,3
```

æŒ‰ crawler å¯¹æ¯”ä¸¤å¤©æ•°æ®ï¼š

```bash
TODAY=$(date +%y%m%d)
YESTERDAY=$(date -d "yesterday" +%y%m%d 2>/dev/null || date -v-1d +%y%m%d)
duckdb -csv -c "
WITH t AS (SELECT crawler, COUNT(*) as cnt FROM 'dump_parquet/all/${TODAY}/**/*.parquet' GROUP BY 1),
     y AS (SELECT crawler, COUNT(*) as cnt FROM 'dump_parquet/all/${YESTERDAY}/**/*.parquet' GROUP BY 1)
SELECT COALESCE(t.crawler, y.crawler) as crawler,
       COALESCE(y.cnt,0) as yesterday, COALESCE(t.cnt,0) as today,
       COALESCE(t.cnt,0) - COALESCE(y.cnt,0) as diff
FROM t FULL OUTER JOIN y ON t.crawler = y.crawler
WHERE ABS(diff) > 100 ORDER BY diff
"
```

## Data Structure

```text
dump_parquet/
â”œâ”€â”€ all/YYMMDD/                      # å…¨é‡çˆ¬è™« (UTC 00:00)
â””â”€â”€ increment/YYMMDD_HHMMSS/         # å¢é‡çˆ¬è™« (UTC 03:00-23:00)

dump_ndjson/
â””â”€â”€ increment/YYMMDD_HHMMSS/         # å¢é‡åŸå§‹æ•°æ®

~/.local/share/pueue/task_logs/      # Pueue ä»»åŠ¡æ—¥å¿— (<ID>.log)
```

## Log Patterns

æ—¥å¿—ä½ç½®ï¼š`~/.local/share/pueue/task_logs/<ID>.log`

å…³é”® rg patternsï¼š

```bash
# çˆ¬å–ç»Ÿè®¡ï¼ˆåœ¨æ—¥å¿—æœ«å°¾ï¼Œä»»åŠ¡å¿«é€Ÿå¤±è´¥æ—¶å¯èƒ½æ— è¾“å‡ºï¼‰
rg -oP "'item_scraped_count': \K\d+"
rg -oP "'spider_exceptions/count': \K\d+"

# ä»£ç†é…ç½®ç¡®è®¤
rg "ProxyMiddleware initialized with JHS_PROXY"
# JHS_PROXY: None â†’ ç›´è¿
# JHS_PROXY: cloudbypass â†’ HTTP ä»£ç†

# é”™è¯¯ç±»å‹
rg "ERROR"
rg "ValueError: Cannot use xpath"      # ä»£ç†è¿”å› JSON è€Œé HTML
rg "Failed to get page_max_value"      # é¦–é¡µè§£æå¤±è´¥
```

## Common Issues

| ç—‡çŠ¶ | åŸå›  | ä¿®å¤ |
| ------ | ------ | ------ |
| æ—  `item_scraped_count` + ä»»åŠ¡ <10s ç»“æŸ | é¦–é¡µè§£æå¤±è´¥ï¼Œæ— åˆ†é¡µç”Ÿæˆ | æ£€æŸ¥æ—¥å¿— ERRORï¼Œç¡®è®¤ç½‘ç«™å¯è®¿é—®æ€§ |
| `item_scraped_count: 0` + `Failed to get page_max_value` + HTTP 200 | **ç½‘ç«™åå°é—®é¢˜**ï¼ˆpage_max ä¸ºç©ºï¼‰ | è”ç³»ç½‘ç«™æ–¹ç¡®è®¤å•†å“æ•°æ®åº“çŠ¶æ€ |
| `item_scraped_count: 0` + `Failed to get page_max_value` + HTTP 403 | Cloudflare å°ç¦ | æ·»åŠ  `-s JHS_PROXY=cloudbypass`ï¼ˆä»…é™é dorasutaï¼‰ |
| `ValueError: Cannot use xpath on Selector of type 'json'` | ä»£ç†è¿”å› 403 JSON | ä»£ç†é—®é¢˜æˆ–éœ€è¦å¢åŠ å®¹é”™ |
| `response_status_count/403` å¾ˆé«˜ | è¢«ç›®æ ‡ç½‘ç«™é™æµ | é™ä½ `CONCURRENT_REQUESTS` |
| å•ä¸ª spider æ•°æ®ä¸º 0ï¼Œå…¶ä»–æ­£å¸¸ | è¯¥ç±»ç›®é¡µé¢ç»“æ„å˜åŒ–æˆ–åå°æ— æ•°æ® | ç”¨ scrapy fetch å¯¹æ¯”é¡µé¢ç»“æ„ |
| hareruya2 è¿”å› "å•†å“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ" | è¯¥ç±»ç›®æ— åœ¨åº“å•†å“ï¼ˆfilter.v.availability=1ï¼‰ | æ­£å¸¸ç°è±¡ï¼Œæ— éœ€å¤„ç† |

## Page Structure Analysis

å½“æ€€ç–‘æ˜¯**é¡µé¢ç»“æ„å˜åŒ–**æˆ–**ç½‘ç«™åå°é—®é¢˜**æ—¶ï¼Œç”¨ scrapy fetch ä¿å­˜é¡µé¢è¿›è¡Œåˆ†æï¼š

```bash
cd /root/python-spider2/apps/spider2

# ä¿å­˜é¡µé¢å†…å®¹ï¼ˆæ¯” scrapy shell æ›´å¯é ï¼Œä¸ä¼šå¡ä½ï¼‰
timeout 30 mise exec --env local -- uv run scrapy fetch "<URL>" -s LOG_LEVEL=ERROR 2>/dev/null > /tmp/page.html

# æ£€æŸ¥å…³é”®å…ƒç´ 
rg -oP '<input[^>]*id="page_max"[^>]*>' /tmp/page.html   # åˆ†é¡µæ•°
rg -c 'class="element"' /tmp/page.html                    # å•†å“æ•°é‡ï¼ˆdorasutaï¼‰
rg -c 'id="product-grid"' /tmp/page.html                  # å•†å“å®¹å™¨ï¼ˆhareruya2ï¼‰
```

### å¯¹æ¯”æ­£å¸¸é¡µé¢å’Œå¼‚å¸¸é¡µé¢

```bash
# ä¿å­˜ä¸¤ä¸ªé¡µé¢ï¼ˆåŒä¸€ç½‘ç«™ä¸åŒç±»ç›®ï¼‰
timeout 30 mise exec --env local -- uv run scrapy fetch "https://dorasuta.jp/pokemon-card/product-list?cocd=1" -s LOG_LEVEL=ERROR 2>/dev/null > /tmp/pokemon.html
timeout 30 mise exec --env local -- uv run scrapy fetch "https://dorasuta.jp/mtg/product-list?cocd=1" -s LOG_LEVEL=ERROR 2>/dev/null > /tmp/mtg.html

# å¯¹æ¯”å…³é”®å…ƒç´ 
echo "Pokemon page_max:"; rg -oP 'id="page_max"[^>]*value="\K[^"]*' /tmp/pokemon.html || echo "(empty)"
echo "MTG page_max:"; rg -oP 'id="page_max"[^>]*value="\K[^"]*' /tmp/mtg.html
echo "Pokemon elements:"; rg -c 'class="element"' /tmp/pokemon.html
echo "MTG elements:"; rg -c 'class="element"' /tmp/mtg.html
```

**è§£è¯»**ï¼šå¦‚æœ MTG æ­£å¸¸ï¼ˆpage_max=6266ï¼‰è€Œ Pokemon ä¸ºç©ºï¼Œè¯´æ˜æ˜¯ Pokemon ç±»ç›®åå°é—®é¢˜ï¼Œä¸æ˜¯çˆ¬è™«é—®é¢˜ã€‚

## Debug Workflow

1. **ç¡®è®¤é—®é¢˜èŒƒå›´**ï¼šæ£€æŸ¥ä»Šæ—¥ç›¸å…³ä»»åŠ¡çš„ item_scraped_count
2. **æŸ¥çœ‹æ—¥å¿—**ï¼šæ‰¾ ERROR æˆ–å¼‚å¸¸ç»Ÿè®¡ï¼ˆæ³¨æ„ï¼šå¿«é€Ÿå¤±è´¥çš„ä»»åŠ¡æ— ç»Ÿè®¡è¾“å‡ºï¼‰
3. **ç¡®è®¤ä»£ç†é…ç½®**ï¼š

   ```bash
   rg "ProxyMiddleware initialized" ~/.local/share/pueue/task_logs/<ID>.log
   ```

4. **åŒºåˆ†é—®é¢˜ç±»å‹**ï¼š
   - HTTP 403 + "Just a moment..." â†’ Cloudflare å°ç¦
   - HTTP 200 + page_max ä¸ºç©º â†’ ç½‘ç«™åå°é—®é¢˜
   - HTTP 200 + xpath åŒ¹é…å¤±è´¥ â†’ é¡µé¢ç»“æ„å˜åŒ–
   - "å•†å“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ" â†’ è¯¥ç±»ç›®æ— åœ¨åº“å•†å“ï¼ˆæ­£å¸¸ï¼‰
5. **å¯¹æ¯”åˆ†æ**ï¼šç”¨ scrapy fetch ä¿å­˜é¡µé¢ï¼Œå¯¹æ¯”æ­£å¸¸/å¼‚å¸¸é¡µé¢
6. **å†å²è¶‹åŠ¿**ï¼šæ£€æŸ¥é—®é¢˜å¼€å§‹æ—¶é—´ï¼Œåˆ¤æ–­æ˜¯çªå‘è¿˜æ˜¯æ¸è¿›

## å¸¸è§è¯¯åˆ¤åŠé¿å…æ–¹æ³•

| è¯¯åˆ¤ | å®é™…æƒ…å†µ | å¦‚ä½•é¿å… |
| ------ | ---------- | ---------- |
| "dorasuta éœ€è¦åŠ  cloudbypass ä»£ç†" | dorasuta æ˜¯ IP ç™½åå•ï¼Œç›´è¿å³å¯ | æ£€æŸ¥ apps/cron/spider2/crawl_all/main.sh |
| "CloudBypass ä½™é¢ä¸è¶³å¯¼è‡´å¤±è´¥" | dorasuta ä¸èµ° CloudBypass API | ç¡®è®¤ä»£ç†æ¨¡å¼ |
| "ç½‘ç«™æ— æ³•è®¿é—®ï¼ˆcurl 403ï¼‰" | curl/httpx è¢« Cloudflare æ‹¦æˆª | ç”¨ scrapy fetch æµ‹è¯• |
| "hareruya.com è¶…æ—¶" | å®é™… URL æ˜¯ hareruya2.com | ä»çˆ¬è™«ä»£ç æå– URL |
| "ä»£ç†é…ç½®é”™è¯¯" | HTTP ä»£ç†å’Œ API ä»£ç†æ··æ·† | åŒºåˆ† JHS_PROXY å’Œ CloudBypassRequest |

## ç¯å¢ƒå˜é‡å‚è€ƒ

### HTTP ä»£ç†æ¨¡å¼ï¼ˆJHS_PROXY=cloudbypassï¼‰

```bash
PROXY_CLOUDBYPASS_SERVER    # ä»£ç†æœåŠ¡å™¨åœ°å€
PROXY_CLOUDBYPASS_USERNAME  # ä»£ç†ç”¨æˆ·å
PROXY_CLOUDBYPASS_PASSWORD  # ä»£ç†å¯†ç 
```

### API ä»£ç†æ¨¡å¼ï¼ˆCloudBypassRequestï¼‰

```bash
PROXY_CLOUDBYPASS_APIKEY    # API Key
PROXY_CLOUDBYPASS_LONG      # ä»£ç†æ± åˆ—è¡¨ï¼ˆ| åˆ†éš”ï¼‰
```

**æ³¨æ„**ï¼šdorasuta ä»£ç è™½æœ‰ CloudBypassRequestï¼Œä½†é»˜è®¤ `cloudbypass_enabled=False`ï¼Œä¸ä½¿ç”¨ API æ¨¡å¼ã€‚

## Historical Trend Analysis

æ£€æŸ¥é—®é¢˜å¼€å§‹æ—¶é—´ï¼Œå®šä½æ˜¯æ¸è¿›æ€§è¿˜æ˜¯çªå‘æ€§ï¼š

```bash
# æŸ¥çœ‹æŸä¸ª spider æœ€è¿‘ 10 æ¬¡çš„ item_scraped_count è¶‹åŠ¿
for id in $(pueue status --json | jq -r '.tasks | to_entries[] | select(.value.original_command | contains("dorasuta_pokemon")) | select(.value.original_command | contains("product_detail") | not) | .key' | tail -10); do
  created=$(pueue status --json | jq -r ".tasks[\"$id\"].created_at" | cut -d'T' -f1)
  cnt=$(rg -oP "'item_scraped_count': \K\d+" ~/.local/share/pueue/task_logs/${id}.log 2>/dev/null | tail -1)
  echo "$created (task $id): ${cnt:-0} items"
done | sort
```

æ‰¹é‡æ£€æŸ¥æœ€è¿‘æ—¥å¿—çš„ exceptionsï¼š

```bash
for id in $(ls -t ~/.local/share/pueue/task_logs/*.log 2>/dev/null | head -20 | xargs -I {} basename {} .log); do
  exc=$(rg -oP "'spider_exceptions/count': \K\d+" ~/.local/share/pueue/task_logs/${id}.log 2>/dev/null | tail -1)
  [ -n "$exc" ] && [ "$exc" -gt 0 ] && echo "$id: $exc exceptions"
done
```

## Quick Checks

```bash
# Cron é…ç½®
crontab -l | rg spider2

# ä»Šæ—¥ä»»åŠ¡æ¦‚è§ˆ
pueue status --json | jq '[.tasks | to_entries[] | select(.value.created_at > "'$(date +%Y-%m-%d)'") | select(.value.path | contains("spider2"))] | length'

# æ£€æŸ¥ç‰¹å®š spider çš„ä»»åŠ¡
pueue status --json | jq '.tasks | to_entries[] | select(.value.original_command | contains("dorasuta_pokemon"))'

# ä»Šæ—¥æ‰€æœ‰ dorasuta ä»»åŠ¡çš„ item_scraped_count
for id in $(pueue status --json | jq -r '.tasks | to_entries[] | select(.value.created_at > "'$(date +%Y-%m-%d)'") | select(.value.original_command | contains("dorasuta")) | .key'); do
  spider=$(pueue status --json | jq -r ".tasks[\"$id\"].original_command" | rg -oP 'crawl \K[^ ]+')
  cnt=$(rg -oP "'item_scraped_count': \K\d+" ~/.local/share/pueue/task_logs/${id}.log 2>/dev/null | tail -1)
  echo "$spider: ${cnt:-0}"
done
```

## å¢é‡çˆ¬è™«è°ƒè¯•

å¢é‡çˆ¬è™«ï¼ˆ`dorasuta_product_detail`ï¼‰æŒ‰ URL çˆ¬å–è¯¦æƒ…é¡µï¼Œä¸å…¨é‡åˆ—è¡¨çˆ¬è™«ä¸åŒã€‚

### å¢é‡æ•°æ®è·¯å¾„

```text
data/product_urls/increment/YYMMDD_HHMMSS/
â”œâ”€â”€ summary.json                           # å„ç±»ç›® URL æ•°é‡
â”œâ”€â”€ dorasuta_pokemon_urls.txt              # Pokemon URL åˆ—è¡¨
â”œâ”€â”€ dorasuta_pokemon_urls_split/           # åˆ†ç‰‡ç›®å½•
â”‚   â”œâ”€â”€ part_0.txt
â”‚   â”œâ”€â”€ part_1.txt
â”‚   â””â”€â”€ ...
â””â”€â”€ dorasuta_yugioh_urls.txt               # YuGiOh URL åˆ—è¡¨
```

### æ£€æŸ¥å¢é‡çˆ¬å–ç»“æœ

```bash
# æŸ¥çœ‹æŸæ—¶æ®µæŸç±»ç›®çš„çˆ¬å–ç»“æœ
for id in $(rg -l "pokemon_260115_060001" ~/.local/share/pueue/task_logs/ 2>/dev/null | xargs -I{} basename {} .log); do
  cnt=$(rg -oP "'item_scraped_count': \K\d+" ~/.local/share/pueue/task_logs/${id}.log | tail -1)
  warn=$(rg -c "æ²¡æœ‰è·å–åˆ°" ~/.local/share/pueue/task_logs/${id}.log)
  echo "Task $id: scraped=${cnt:-0}, warnings=$warn"
done
```

### åˆ¤æ–­å¢é‡å¤±è´¥ç±»å‹

**è­¦å‘Šæ•° vs æŠ“å–æ•°å…³ç³»**ï¼š

| è­¦å‘Šæ•° | å«ä¹‰ | åŸå›  |
| :------- | :----- | :----- |
| `warnings â‰ˆ scraped Ã— 2` | å…¨éƒ¨å¤±è´¥ | æ¯æ¡å•†å“ 2 ä¸ªè­¦å‘Šï¼ˆæ¡ä»¶åˆ—è¡¨ + ç³»åˆ—åˆ—è¡¨ï¼‰ |
| `warnings << scraped Ã— 2` | å¤§éƒ¨åˆ†æˆåŠŸ | å°‘é‡å•†å“ä¸‹æ¶æˆ–æ•°æ®å¼‚å¸¸ |
| `warnings = 0` | å…¨éƒ¨æˆåŠŸ | æ­£å¸¸ |

### å¢é‡ Parquet æŒ‰ç±»ç›®ç»Ÿè®¡

```sql
-- æ£€æŸ¥å“ªäº›ç±»ç›®æœ‰ç©ºæ ‡é¢˜ï¼ˆå¤±è´¥æ ‡å¿—ï¼‰
SELECT
  CASE
    WHEN product_url LIKE '%pokemon%' THEN 'pokemon'
    WHEN product_url LIKE '%yugioh%' THEN 'yugioh'
    WHEN product_url LIKE '%mtg%' THEN 'mtg'
    ELSE 'other'
  END as game,
  COUNT(*) as total,
  SUM(CASE WHEN product_title = '' OR product_title IS NULL THEN 1 ELSE 0 END) as empty_title
FROM 'dump_parquet/increment/260115_*/**/*.parquet'
GROUP BY 1
ORDER BY empty_title DESC
```

**è§£è¯»**ï¼š`empty_title = total` è¡¨ç¤ºè¯¥ç±»ç›®å…¨éƒ¨å¤±è´¥ï¼ˆåå°ç»´æŠ¤ï¼‰ï¼›`empty_title` å æ¯”å°è¡¨ç¤ºéƒ¨åˆ†å•†å“ä¸‹æ¶ã€‚

### condition_list å­—æ®µæ’æŸ¥

å½“å‘ç° `condition_list` ä¸ºç©ºæ—¶ï¼ŒæŒ‰ä»¥ä¸‹é¡ºåºæ’æŸ¥ï¼š

```bash
# 1. å…ˆæ£€æŸ¥ title æ˜¯å¦ä¹Ÿä¸ºç©ºï¼ˆå¿«é€Ÿåˆ¤æ–­åå°ç»´æŠ¤ï¼‰
duckdb -csv -c "
SELECT
  regexp_extract(product_url, 'dorasuta\.jp/([^/]+)/', 1) as game,
  SUM(CASE WHEN len(product_condition_list) = 0 THEN 1 ELSE 0 END) as empty_condition,
  SUM(CASE WHEN product_title = '' THEN 1 ELSE 0 END) as empty_title
FROM 'dump_parquet/increment/<RUN_ID>/**/*.parquet'
GROUP BY 1 HAVING empty_condition > 0
"
```

| empty_condition | empty_title | ç»“è®º |
| :---------------- | :------------ | :----- |
| N | N | åå°ç»´æŠ¤ï¼Œæ— éœ€ä¿®å¤ |
| N | 0 | **è§£æé€»è¾‘é—®é¢˜**ï¼Œéœ€æ£€æŸ¥ xpath |
| å°‘é‡ | 0 | éƒ¨åˆ†å•†å“æ— åº“å­˜ï¼ˆåœ¨åº«ãªã—ï¼‰ï¼Œæ­£å¸¸ |

```bash
# 2. å¦‚æœ empty_title = 0ï¼Œæ£€æŸ¥ condition å€¼åˆ†å¸ƒ
duckdb -csv -c "
SELECT unnest(product_condition_list).condition as cond, COUNT(*)
FROM 'dump_parquet/increment/<RUN_ID>/**/*.parquet'
GROUP BY 1 ORDER BY 2 DESC
"
# æ­£å¸¸å€¼: çŠ¶æ…‹A, çŠ¶æ…‹B, çŠ¶æ…‹C, çŠ¶æ…‹Aç‰¹ä¾¡, çŠ¶æ…‹Bç‰¹ä¾¡, çŠ¶æ…‹Cç‰¹ä¾¡
```

## åå°ç»´æŠ¤ vs å•†å“ä¸‹æ¶

é¾™æ˜Ÿåå°ä¼šè½®æ¢ç»´æŠ¤ä¸åŒç±»ç›®ï¼Œå¯¼è‡´ä¸åŒæ—¶æ®µä¸åŒç±»ç›®å¯èƒ½å¤±è´¥ã€‚

### ä¸¤ç§åœºæ™¯åŒºåˆ†

| åœºæ™¯ | å½±å“èŒƒå›´ | åˆ—è¡¨é¡µ | è¯¦æƒ…é¡µ | æ—¥å¿—ç‰¹å¾ |
| :----- | :--------- | :------- | :------- | :--------- |
| **åå°ç»´æŠ¤** | æ•´ä¸ªç±»ç›® 100% | `page_max` ä¸ºç©º | æ—  `frame product` | å…¨é‡ï¼š`Failed to get page_max_value` å¢é‡ï¼šæ¯æ¡éƒ½æœ‰ 2 ä¸ªè­¦å‘Š |
| **å•†å“ä¸‹æ¶** | å•ä¸ªå•†å“ | æ­£å¸¸ | æ—  `frame product` | ä»…ä¸ªåˆ«å•†å“æœ‰è­¦å‘Š |

### åå°ç»´æŠ¤éªŒè¯

```bash
# å¯¹æ¯”åŒä¸€ç½‘ç«™ä¸åŒç±»ç›®çš„ page_max
cd apps/spider2
for game in pokemon-card mtg yugioh-jp weissschwarz; do
  cnt=$(timeout 20 mise exec --env local -- uv run scrapy fetch "https://dorasuta.jp/${game}/product-list?cocd=1" -s LOG_LEVEL=ERROR 2>/dev/null | rg -oP 'id="page_max"[^>]*value="\K[^"]*')
  echo "$game: ${cnt:-(empty)}"
done
```

**è§£è¯»**ï¼šå¦‚æœéƒ¨åˆ†ç±»ç›®æ­£å¸¸ã€éƒ¨åˆ†ä¸ºç©º â†’ é¾™æ˜Ÿåå°è½®æ¢ç»´æŠ¤

### å•†å“ä¸‹æ¶éªŒè¯

```bash
# æ£€æŸ¥è¯¦æƒ…é¡µæ˜¯å¦æœ‰ frame product
cd apps/spider2
mise exec --env local -- uv run scrapy fetch "https://dorasuta.jp/pokemon-card/product?pid=654882" -s LOG_LEVEL=ERROR 2>/dev/null > /tmp/detail.html

# frame product å­˜åœ¨ = æ­£å¸¸å•†å“
rg -c 'class="frame product"' /tmp/detail.html  # 0=ä¸‹æ¶, 1=æ­£å¸¸

# é¡µé¢è¡Œæ•°å¯¹æ¯”ï¼ˆä¸‹æ¶çº¦1200è¡Œï¼Œæ­£å¸¸çº¦3500è¡Œï¼‰
wc -l /tmp/detail.html
```

### å®šä½æ¢å¤æ—¶é—´ç‚¹

å…¨é‡å¤±è´¥æ—¶ï¼Œæ£€æŸ¥å¢é‡ä»å“ªä¸ªæ—¶æ®µå¼€å§‹æ¢å¤ï¼š

```bash
for run in 030001 033001 040001 043001 050001 053001; do
  logs=$(rg -l "pokemon_260115_${run}" ~/.local/share/pueue/task_logs/ 2>/dev/null)
  [ -z "$logs" ] && continue
  warn=0; scraped=0
  for f in $logs; do
    w=$(rg -c "æ²¡æœ‰è·å–åˆ°" $f 2>/dev/null)
    c=$(rg -oP "'item_scraped_count': \K\d+" $f | tail -1)
    warn=$((warn + w))
    scraped=$((scraped + ${c:-0}))
  done
  # è­¦å‘Šæ•°è¿œå°äº scraped è¯´æ˜å·²æ¢å¤
  if [ $warn -lt $scraped ]; then
    echo "æ¢å¤æ—¶é—´ç‚¹: 260115_${run} (scraped=$scraped, warnings=$warn)"
    break
  else
    echo "260115_${run}: ä»å¤±è´¥ (scraped=$scraped, warnings=$warn)"
  fi
done
```
